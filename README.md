MoiraiLM ğŸŒŸ
 
MoiraiLM æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºä»é›¶æ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ğŸš€ã€‚å®ƒå¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£å’ŒæŒæ¡ LLM çš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬åˆ†è¯å™¨ã€Transformer æ¶æ„ã€è®­ç»ƒå’Œæ¨ç†ï¼Œé€šè¿‡æœ€å°åŒ–å’Œå¹²å‡€çš„å®ç°æ¥å®ç°ã€‚è¯¥åç§°â€œMoiraiâ€æºè‡ªå¸Œè…Šç¥è¯ä¸­ä¸‰ä½å‘½è¿å¥³ç¥ï¼Œè±¡å¾ç€å¯¹è¯­è¨€æµåŠ¨å’Œç»“æœçš„æŒæ§ï¼Œç±»ä¼¼äº LLM çš„ç”Ÿæˆèƒ½åŠ›ã€‚
English Version ğŸ‡¬ğŸ‡§
Project Overview ğŸ“–
MoiraiLM is an open-source framework for building large language models (LLMs) from scratch. It helps developers understand and master the core components of LLMs, including tokenizer, transformer architecture, training, and inference, through minimal and clean implementations. The name "Moirai" is inspired by the three goddesses of fate in Greek mythology, symbolizing control over the flow and outcome of language, much like LLMs.
Features âœ¨

âœï¸ Handwritten implementation of all LLM components from scratch.
ğŸŒ Custom BPE tokenizer supporting English and Chinese text.
ğŸ› ï¸ Clean transformer code with support for RoPE (Rotary Position Embedding) and RMSNorm.
ğŸ“š Minimal training and inference loops designed for educational purposes.
ğŸ”— Full-stack design covering tokenizer, model, training, and inference.

Project Structure ğŸ“‚
The repository is organized as follows:
MoiraiLM/
â”œâ”€â”€ tokenizer/         # BPE tokenizer implementation ğŸ—£ï¸
â”œâ”€â”€ model/             # Transformer architecture ğŸ§ 
â”œâ”€â”€ data/              # Data loading & preprocessing ğŸ“Š
â”œâ”€â”€ train/             # Training scripts ğŸ‹ï¸
â”œâ”€â”€ inference/         # Text generation ğŸ“
â”œâ”€â”€ utils/             # Helpers and config files ğŸ› ï¸
â”œâ”€â”€ examples/          # Usage examples and notebooks ğŸ““
â”œâ”€â”€ requirements.txt   # Dependencies ğŸ“¦
â”œâ”€â”€ configs/           # Configuration files (e.g., train_config.yaml) âš™ï¸
â””â”€â”€ README.md

Getting Started ğŸš€
1. Clone the Repository ğŸ“¥
git clone https://github.com/colinwps/MoiraiLM.git
cd MoiraiLM

2. Install Dependencies ğŸ› ï¸
pip install -r requirements.txt

(Note: Dependencies include PyTorch, NumPy, and other standard libraries for ML.)
3. Train BPE Tokenizer ğŸ“
Train the tokenizer on your corpus:
python tokenizer/train_bpe.py --input data/corpus.txt --vocab_size 5000


data/corpus.txt: Your training text corpus (supports mixed English/Chinese).
vocab_size: Desired vocabulary size (e.g., 5000).

4. Train the Model ğŸ‹ï¸
python train/train_lm.py --config configs/train_config.yaml


Customize hyperparameters in configs/train_config.yaml (e.g., batch size, learning rate, model dimensions).

5. Run Inference ğŸ“
Generate text based on a prompt:
python inference/generate.py --prompt "Today is a beautiful day"

Example outputs:

Input: "Today is a beautiful day" â†’ Output: ", perfect for a walk in the park." ğŸŒ³
Input: "The capital of China is" â†’ Output: "Beijing." ğŸ‡¨ğŸ‡³

Examples ğŸ““
Check the examples/ directory for Jupyter notebooks demonstrating:

Tokenizer training on custom data ğŸ—£ï¸
Fine-tuning the model on specific datasets ğŸ”§
Advanced inference techniques like beam search ğŸ”

Contributing ğŸ¤
We welcome contributions! 

ğŸ› Report bugs or request features via Issues.
ğŸ’¡ Submit pull requests for improvements.
â­ Star the repo to show your support.

License ğŸ“œ
This project is licensed under the MIT License - see the LICENSE file for details.
Contact ğŸ“¬

GitHub: @colinwps
WeChat Official Account: å†™ä»£ç çš„ä¸­å¹´äºº (The Middle-Aged Programmer)

"Destiny is not written â€” itâ€™s generated, token by token." âœ¨

ä¸­æ–‡ç‰ˆ ğŸ‡¨ğŸ‡³
é¡¹ç›®æ¦‚è¿° ğŸ“–
MoiraiLM æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œç”¨äºä»é›¶æ„å»ºå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) ğŸš€ã€‚å®ƒå¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£å’ŒæŒæ¡ LLM çš„æ ¸å¿ƒç»„ä»¶ï¼ŒåŒ…æ‹¬åˆ†è¯å™¨ã€Transformer æ¶æ„ã€è®­ç»ƒå’Œæ¨ç†ï¼Œé€šè¿‡æœ€å°åŒ–å’Œå¹²å‡€çš„å®ç°ã€‚è¯¥åç§°â€œMoiraiâ€æºè‡ªå¸Œè…Šç¥è¯ä¸­ä¸‰ä½å‘½è¿å¥³ç¥ï¼Œè±¡å¾ç€å¯¹è¯­è¨€æµåŠ¨å’Œç»“æœçš„æŒæ§ï¼Œç±»ä¼¼äº LLM çš„ç”Ÿæˆèƒ½åŠ›ã€‚
ç‰¹æ€§ âœ¨

âœï¸ ä»é›¶æ‰‹å†™å®ç°æ‰€æœ‰ LLM ç»„ä»¶ã€‚
ğŸŒ è‡ªå®šä¹‰ BPE åˆ†è¯å™¨ï¼Œæ”¯æŒè‹±æ–‡å’Œä¸­æ–‡æ–‡æœ¬ã€‚
ğŸ› ï¸ å¹²å‡€çš„ Transformer ä»£ç ï¼Œæ”¯æŒ RoPE (æ—‹è½¬ä½ç½®åµŒå…¥) å’Œ RMSNormã€‚
ğŸ“š æœ€å°åŒ–çš„è®­ç»ƒå’Œæ¨ç†å¾ªç¯ï¼Œä¸“ä¸ºæ•™è‚²ç›®çš„è®¾è®¡ã€‚
ğŸ”— å…¨æ ˆè®¾è®¡ï¼Œæ¶µç›–åˆ†è¯å™¨ã€æ¨¡å‹ã€è®­ç»ƒå’Œæ¨ç†ã€‚

é¡¹ç›®ç»“æ„ ğŸ“‚
ä»“åº“ç»“æ„å¦‚ä¸‹ï¼š
MoiraiLM/
â”œâ”€â”€ tokenizer/         # BPE åˆ†è¯å™¨å®ç° ğŸ—£ï¸
â”œâ”€â”€ model/             # Transformer æ¶æ„ ğŸ§ 
â”œâ”€â”€ data/              # æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ğŸ“Š
â”œâ”€â”€ train/             # è®­ç»ƒè„šæœ¬ ğŸ‹ï¸
â”œâ”€â”€ inference/         # æ–‡æœ¬ç”Ÿæˆ ğŸ“
â”œâ”€â”€ utils/             # è¾…åŠ©å·¥å…·å’Œé…ç½®æ–‡ä»¶ ğŸ› ï¸
â”œâ”€â”€ examples/          # ä½¿ç”¨ç¤ºä¾‹å’Œç¬”è®°æœ¬ ğŸ““
â”œâ”€â”€ requirements.txt   # ä¾èµ–é¡¹ ğŸ“¦
â”œâ”€â”€ configs/           # é…ç½®æ–‡ä»¶ (ä¾‹å¦‚ train_config.yaml) âš™ï¸
â””â”€â”€ README.md

å¿«é€Ÿå¼€å§‹ ğŸš€
1. å…‹éš†ä»“åº“ ğŸ“¥
git clone https://github.com/colinwps/MoiraiLM.git
cd MoiraiLM

2. å®‰è£…ä¾èµ– ğŸ› ï¸
pip install -r requirements.txt

(æ³¨æ„ï¼šä¾èµ–åŒ…æ‹¬ PyTorchã€NumPy ç­‰æœºå™¨å­¦ä¹ æ ‡å‡†åº“ã€‚)
3. è®­ç»ƒ BPE åˆ†è¯å™¨ ğŸ“
åœ¨è¯­æ–™åº“ä¸Šè®­ç»ƒåˆ†è¯å™¨ï¼š
python tokenizer/train_bpe.py --input data/corpus.txt --vocab_size 5000


data/corpus.txtï¼šæ‚¨çš„è®­ç»ƒæ–‡æœ¬è¯­æ–™åº“ï¼ˆæ”¯æŒä¸­è‹±æ–‡æ··åˆï¼‰ã€‚
vocab_sizeï¼šæœŸæœ›çš„è¯æ±‡è¡¨å¤§å°ï¼ˆä¾‹å¦‚ 5000ï¼‰ã€‚

4. è®­ç»ƒæ¨¡å‹ ğŸ‹ï¸
python train/train_lm.py --config configs/train_config.yaml


åœ¨ configs/train_config.yaml ä¸­è‡ªå®šä¹‰è¶…å‚æ•°ï¼ˆä¾‹å¦‚æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ã€æ¨¡å‹ç»´åº¦ï¼‰ã€‚

5. è¿è¡Œæ¨ç† ğŸ“
åŸºäºæç¤ºç”Ÿæˆæ–‡æœ¬ï¼š
python inference/generate.py --prompt "ä»Šå¤©å¤©æ°”çœŸå¥½"

ç¤ºä¾‹è¾“å‡ºï¼š

è¾“å…¥ï¼š"ä»Šå¤©å¤©æ°”çœŸå¥½" â†’ è¾“å‡ºï¼š"ï¼Œé€‚åˆå‡ºå»æ•£æ­¥ã€‚" ğŸŒ³
è¾“å…¥ï¼š"ä»Šå¤©å¤©æ°”å¥½å†·" â†’ è¾“å‡ºï¼š"é‚£åº”è¯¥è¦åŠ è¡£æœäº†" ğŸ‡¨ğŸ‡³

ç¤ºä¾‹ ğŸ““
æŸ¥çœ‹ examples/ ç›®å½•ä¸­çš„ Jupyter ç¬”è®°æœ¬ï¼Œæ¼”ç¤ºï¼š

åœ¨è‡ªå®šä¹‰æ•°æ®ä¸Šè®­ç»ƒåˆ†è¯å™¨ ğŸ—£ï¸
åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ ğŸ”§
é«˜çº§æ¨ç†æŠ€æœ¯ï¼Œå¦‚æŸæœç´¢ ğŸ”

è´¡çŒ® ğŸ¤
æ¬¢è¿è´¡çŒ®ï¼

ğŸ› é€šè¿‡ Issues æŠ¥å‘Š bug æˆ–è¯·æ±‚åŠŸèƒ½ã€‚
ğŸ’¡ æäº¤æ‹‰å–è¯·æ±‚ä»¥æ”¹è¿›ä»£ç ã€‚
â­ ç»™ä»“åº“åŠ æ˜Ÿä»¥æ”¯æŒå¼€å‘ã€‚

è®¸å¯ ğŸ“œ
æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯ - è¯¦è§ LICENSE æ–‡ä»¶ã€‚
è”ç³»æ–¹å¼ ğŸ“¬

GitHub: @colinwps
å¾®ä¿¡å…¬ä¼—å·ï¼šå†™ä»£ç çš„ä¸­å¹´äºº

â€œå‘½è¿å¹¶éä¹¦å†™è€Œæˆâ€”â€”å®ƒæ˜¯ç”±ä¸€ä¸ªä¸€ä¸ª token ç”Ÿæˆçš„ã€‚â€ âœ¨