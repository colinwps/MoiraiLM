# path: scripts/train_gpt_RoPE.py (ä¼˜åŒ–ç‰ˆ)
"""
è®­ç»ƒç®€åŒ–ç‰ˆ GPT æ¨¡å‹ï¼Œä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰
ç”¨æ³•ï¼š
    python scripts/train_gpt_RoPE.py workdir/spm_wiki.model data/cleaned_wiki_full.txt workdir/gpt_100M_RoPE.pth
"""

import sys
import os
import re
import math
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sentencepiece as spm
from tqdm import tqdm

# --- å…¨å±€å¸¸é‡å’Œè¶…å‚æ•° (ä¿®æ”¹åçš„) ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BLOCK_SIZE = 512  # å¢åŠ åºåˆ—é•¿åº¦ä»¥æ•æ‰é•¿ç¨‹ä¾èµ–
BATCH_SIZE = 32  # å¢åŠ æ‰¹æ¬¡å¤§å° (å¦‚æœæ˜¾å­˜å…è®¸)
EPOCHS = 1  # äº¿çº§å‚æ•°æ¨¡å‹é¢„è®­ç»ƒé€šå¸¸åªéœ€è¦ 1-3 ä¸ª Epoch
LR = 3e-4
# æ¨¡å‹é…ç½®ï¼šç„å‡† 1 äº¿å‚æ•°å·¦å³
MODEL_DIM = 768
N_LAYERS = 12
NUM_HEADS = 12
# ------------------------------------

# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒå’Œ TF32 (A100/H100/4090 ç­‰ç°ä»£å¡)
if DEVICE == 'cuda':
    torch.backends.cuda.matmul.allow_tf32 = True


# ... clean_text å‡½æ•°ä¿æŒä¸å˜ ...
def clean_text(text: str) -> str:
    allowed = re.compile(r"[^\u4e00-\u9fffã€‚ï¼Œã€ï¼ï¼Ÿï¼šï¼›ï¼ˆï¼‰ã€Šã€‹â€”â€”â€¦\n ]+")
    text = allowed.sub(" ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


class TextDataset(Dataset):
    def __init__(self, token_ids: list, block_size: int):
        self.ids = token_ids
        self.block_size = block_size

    def __len__(self):
        # å‡å» block_size + 1 ä»¥ç¡®ä¿èƒ½å–å‡º x å’Œ y
        return max(0, len(self.ids) - self.block_size)

    def __getitem__(self, idx):
        x = torch.tensor(self.ids[idx: idx + self.block_size], dtype=torch.long)
        y = torch.tensor(self.ids[idx + 1: idx + 1 + self.block_size], dtype=torch.long)
        return x, y


# --- RoPE æ—‹è½¬å‡½æ•°ä¼˜åŒ–ï¼šä½¿ç”¨ torch.complex ---
def apply_rope(x, freqs):
    # x: (B, T, H, D) -> (B*T*H, D)
    # freqs: (T, D)

    # 1. è½¬æ¢ä¸ºå¤æ•°å½¢å¼ (D/2 ä¸ªå¤æ•°)
    x_reshaped = x.float().view(*x.shape[:-1], -1, 2)
    x_complex = torch.view_as_complex(x_reshaped)

    freqs_reshaped = freqs.view(-1, freqs.shape[-1] // 2)
    freqs_complex = torch.view_as_complex(freqs_reshaped)

    # 2. å¹¿æ’­å¹¶åº”ç”¨æ—‹è½¬ (ç‚¹ä¹˜å¤æ•°å³å¯å®Œæˆæ—‹è½¬)
    # T ç»´åº¦é€šè¿‡å¹¿æ’­å¯¹é½
    x_rotated_complex = x_complex * freqs_complex[:x.shape[1], None, :]

    # 3. è½¬æ¢å›å®æ•°å½¢å¼å¹¶å±•å¹³
    x_rotated_reshaped = torch.view_as_real(x_rotated_complex)
    x_rotated = x_rotated_reshaped.flatten(start_dim=-2).type_as(x)
    return x_rotated


class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, block_size, attn_dropout=0.1):
        super().__init__()
        assert embed_dim % num_heads == 0
        self.head_dim = embed_dim // num_heads
        self.num_heads = num_heads
        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)  # æƒ¯ä¾‹ä¸åŠ åç½®
        self.out = nn.Linear(embed_dim, embed_dim, bias=False)
        self.drop = nn.Dropout(attn_dropout)

        # é¢„è®¡ç®— RoPE é¢‘ç‡
        self.register_buffer("freqs", self._create_freqs_buffer(block_size, self.head_dim))

    def _create_freqs_buffer(self, block_size, head_dim):
        pos = torch.arange(block_size, dtype=torch.float32)
        dim = torch.arange(0, head_dim, 2, dtype=torch.float32)
        inv_freq = 1.0 / (10000 ** (dim / head_dim))
        freqs = torch.outer(pos, inv_freq)
        # å°† cos å’Œ sin å †å å¹¶å±•å¹³ (ç”¨äº apply_rope ç®€åŒ–ç‰ˆ)
        return torch.stack([torch.cos(freqs), torch.sin(freqs)], dim=-1).flatten(-2)

    def forward(self, x):
        B, T, C = x.shape
        qkv = self.qkv(x).chunk(3, dim=-1)
        # q, k, v: (B, H, T, D_head)
        q, k, v = [t.view(B, T, self.num_heads, self.head_dim).transpose(1, 2) for t in qkv]

        # åº”ç”¨ RoPE åˆ° q å’Œ k (ç»´åº¦éœ€è¦è½¬æ¢ä¸º B, T, H, D)
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)

        q = apply_rope(q, self.freqs[:T].to(q.device))
        k = apply_rope(k, self.freqs[:T].to(k.device))

        # é‡æ–°è½¬æ¢å› (B, H, T, D_head)
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)

        # MatMul Attention
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))

        # Causal Mask
        mask = torch.tril(torch.ones(T, T, device=x.device)).view(1, 1, T, T)
        att = att.masked_fill(mask == 0, float("-inf"))

        att = torch.softmax(att, dim=-1)
        att = self.drop(att)
        out = att @ v  # (B, H, T, D_head)

        # åˆå¹¶å¤´å¹¶è¾“å‡º
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return self.out(out)


class TransformerBlock(nn.Module):
    def __init__(self, dim, num_heads, block_size, dropout=0.1):
        super().__init__()
        self.ln1 = nn.LayerNorm(dim)
        self.attn = MultiHeadSelfAttention(dim, num_heads, block_size, dropout)
        self.ln2 = nn.LayerNorm(dim)
        # FeedForward ç»´åº¦é€šå¸¸è®¾ç½®ä¸º 4 * dim
        self.ff = FeedForward(dim, dim * 4, dropout)

    def forward(self, x):
        # ä½¿ç”¨ Post-LN (GPT-style)
        x = x + self.attn(self.ln1(x))
        x = x + self.ff(self.ln2(x))
        return x


class GPTLike(nn.Module):
    def __init__(self, vocab_size, block_size, n_layers=N_LAYERS, dim=MODEL_DIM, num_heads=NUM_HEADS):
        super().__init__()
        self.token_emb = nn.Embedding(vocab_size, dim)
        self.blocks = nn.ModuleList([
            TransformerBlock(dim, num_heads, block_size)
            for _ in range(n_layers)
        ])
        self.ln = nn.LayerNorm(dim)
        self.head = nn.Linear(dim, vocab_size, bias=False)  # æƒ¯ä¾‹ä¸åŠ åç½®
        self.block_size = block_size
        self.apply(self._init_weights)

        # æ‰“å°æ¨¡å‹å‚æ•°é‡
        n_params = sum(p.numel() for p in self.parameters())
        print(f"æ¨¡å‹å‚æ•°é‡: {n_params / 1e6:.2f} M")  # æ‰“å°å‚æ•°é‡ï¼Œä»¥ M (ç™¾ä¸‡) ä¸ºå•ä½

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            # æƒé‡åˆå§‹åŒ–
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.ones_(module.weight)
            torch.nn.init.zeros_(module.bias)

    def forward(self, idx):
        x = self.token_emb(idx)  # (B, T, C)
        for block in self.blocks:
            x = block(x)
        x = self.ln(x)
        return self.head(x)


def train(model, dataset, epochs=EPOCHS, batch_size=BATCH_SIZE, lr=LR):
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)  # å¢åŠ  workers
    opt = torch.optim.AdamW(model.parameters(), lr=lr)
    loss_fn = nn.CrossEntropyLoss()
    model.train()
    step = 0

    # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = torch.cuda.amp.GradScaler()

    for ep in range(epochs):
        total_loss = 0
        pbar = tqdm(loader, desc=f"Epoch {ep + 1}/{epochs}")
        for xb, yb in pbar:
            xb, yb = xb.to(DEVICE), yb.to(DEVICE)

            # 1. å‰å‘ä¼ æ’­ (åœ¨ autocast åŒºåŸŸå†…)
            with torch.cuda.amp.autocast():
                logits = model(xb)
                loss = loss_fn(logits.view(-1, logits.size(-1)), yb.view(-1))

            # 2. åå‘ä¼ æ’­å’Œä¼˜åŒ– (ä½¿ç”¨ GradScaler)
            opt.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(opt)
            scaler.update()

            total_loss += loss.item()
            step += 1
            if step % 100 == 0:
                pbar.set_postfix(loss=f"{loss.item():.4f}")

        avg_loss = total_loss / len(loader)
        ppl = math.exp(avg_loss)
        print(f"[Epoch {ep + 1}] Avg Loss {avg_loss:.4f} | PPL {ppl:.2f}")


def main():
    if len(sys.argv) < 4:
        print("ç”¨æ³•: python scripts/train_gpt_RoPE.py åˆ†è¯å™¨æ¨¡å‹ è¾“å…¥è¯­æ–™ è¾“å‡ºæ¨¡å‹")
        sys.exit(1)

    sp_model, corpus_path, out_path = sys.argv[1], sys.argv[2], sys.argv[3]
    sp = spm.SentencePieceProcessor(model_file=sp_model)

    # --- æ•°æ®åŠ è½½ä¼˜åŒ–ï¼šé¿å…å†…å­˜æº¢å‡º ---
    print("â³ æ­£åœ¨åŠ è½½å’Œç¼–ç è¯­æ–™...")
    # å‡è®¾ä½ çš„ cleaned_wiki_full.txt æ˜¯ä¸€ä¸ªå·¨å¤§çš„æ–‡ä»¶ï¼Œæˆ‘ä»¬ä¸€æ¬¡æ€§è¯»å–å¹¶ç¼–ç ï¼Œ
    # ä½†åœ¨å®é™…äº¿çº§æ•°æ®è®­ç»ƒä¸­ï¼Œä½ å¯èƒ½éœ€è¦ä¿®æ”¹è¿™é‡Œä½¿ç”¨å†…å­˜æ˜ å°„æˆ–åˆ†å—è¯»å–ã€‚
    # å¯¹äº 2.35 äº¿ Tokenï¼Œä¸€æ¬¡æ€§åŠ è½½æ˜¯å¯è¡Œçš„ã€‚

    with open(corpus_path, encoding="utf-8") as f:
        text = clean_text(f.read())
    ids = sp.encode(text, out_type=int)
    print(f"âœ… è¯­æ–™ç¼–ç å®Œæˆã€‚æ€» Token æ•°: {len(ids)}")

    dataset = TextDataset(ids, BLOCK_SIZE)

    # åˆ›å»ºæ¨¡å‹ï¼Œä½¿ç”¨æ–°çš„é«˜å‚æ•°é…ç½®
    model = GPTLike(sp.get_piece_size(), BLOCK_SIZE).to(DEVICE)
    print(f"ğŸš€ å¼€å§‹åœ¨ {DEVICE} ä¸Šè®­ç»ƒæ¨¡å‹...")

    train(model, dataset)

    # ç¡®ä¿åœ¨ä¿å­˜å‰å°†æ¨¡å‹åˆ‡æ¢å› CPU å†…å­˜
    torch.save(model.state_dict(), out_path)
    print(f"æ¨¡å‹å·²ä¿å­˜: {out_path}")


if __name__ == "__main__":
    main()